{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'The president of United States is a man of the people. He is a man of the people. He is a man of the people. He is a man of the people. He is a man of the people. He is a man of the'}]\n"
     ]
    }
   ],
   "source": [
    "resposta =  pipe(\"The president of United States is\", max_length=50, do_sample=False)\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oie\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import FakeListLLM\n",
    "\n",
    "\n",
    "fake_llm = FakeListLLM(responses=[\"Oie\"])\n",
    "\n",
    "prompt = \"Bom dia\"\n",
    "print(fake_llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Pourquoi les compagnies pétrolières se retirent des énergies vertes. Les investisseurs ont récompensé les géants pétroliers comme Exxon Mobil qui n'ont pas adopté l'éolien et le solaire, lesquels ont été moins rentables ces dernières années que le pétrole et le gaz.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 71, 'total_tokens': 132, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a7d06e42a7', 'finish_reason': 'stop', 'logprobs': None} id='run-6991ec92-3aa1-4484-8a25-1972093e5739-0' usage_metadata={'input_tokens': 71, 'output_tokens': 61, 'total_tokens': 132, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Pourquoi les compagnies pétrolières se retirent des énergies vertes. Les investisseurs ont récompensé les géants pétroliers comme Exxon Mobil qui n'ont pas adopté l'éolien et le solaire, lesquels ont été moins rentables ces dernières années que le pétrole et le gaz.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fastapi import APIRouter\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "input = {\"input\": \"Why Oil Companies Are Walking Back From Green Energy. Investors have rewarded oil giants like Exxon Mobil that did not embrace wind and solar, which have been less profitable in recent years than oil and gas.\"}\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are an English to French translator. Reject any other language.\"),\n",
    "    (\"user\", \"Translate this: {input}\")\n",
    "])\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "response = llm.invoke(template.format_messages(input=input))\n",
    "print(response)\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
